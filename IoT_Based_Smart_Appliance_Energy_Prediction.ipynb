{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.interpolate import interp1d\n",
        "import statsmodels.api as sm\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LSTM, Activation\n",
        "from keras.utils import pad_sequences\n"
      ],
      "metadata": {
        "id": "NOCkXzJeGF_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#cell to import additional libraries or define helper functions\n",
        "from google.colab import files\n",
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)\n",
        "PYTHONHASHSEED = 0\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "import io"
      ],
      "metadata": {
        "id": "0nB0Gb2DnKqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file upload\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4liFfYVRGG-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is csv file from UCI Repo \"[Appliances Energy Prediction](https://archive.ics.uci.edu/dataset/374/appliances+energy+prediction)\" and manually added descriptions from [git link](https://github.com/LuisM78/Appliances-energy-prediction-data/blob/master/variables%20description.txt)\n",
        "\n",
        "Location: Chièvres\n",
        "7950 Chievres\n",
        "Belgium\n"
      ],
      "metadata": {
        "id": "hPldW1x6HNOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of the uploaded files\n",
        "for filename in uploaded.keys():\n",
        "    print(f'User uploaded file \"{filename}\" with length {len(uploaded[filename])} bytes')"
      ],
      "metadata": {
        "id": "en5ia90JIbij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Above is csv file from UCI Repo and manually added descriptions from\n",
        "\n",
        "# Read the uploaded file into a DataFrame\n",
        "df_raw = pd.read_csv('energydata_transformed.csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(df_raw.head())\n"
      ],
      "metadata": {
        "id": "O82SrF5pGbUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy to preserve the original format\n",
        "df = df_raw.copy()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "yYyAWxfCJYbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "def _plot_series(series, series_name, series_index=0):\n",
        "  palette = list(sns.palettes.mpl_palette('Dark2'))\n",
        "  xs = series['Timestamp']\n",
        "  ys = series['Office Room °C']\n",
        "\n",
        "  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')\n",
        "df_sorted = _df_12.sort_values('Timestamp', ascending=True)\n",
        "for i, (series_name, series) in enumerate(df_sorted.groupby('Timestamp')):\n",
        "  _plot_series(series, series_name, i)\n",
        "  fig.legend(title='Timestamp', bbox_to_anchor=(1, 1), loc='upper left')\n",
        "sns.despine(fig=fig, ax=ax)\n",
        "plt.xlabel('Timestamp')\n",
        "_ = plt.ylabel('Office Room °C')"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "L1HaMINmJ5aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "_df_8.plot(kind='scatter', x='Living Room RH %', y='Laundry RH %', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "BDaL0jmPJ1py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "\n",
        "_df_3['Living Room RH %'].plot(kind='hist', bins=20, title='Living Room RH %')\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "gN-F9Op6Jtqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "aY_6kHYpJj2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "Z9xNsYB0KmQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'Timestamp' column to datetime with the correct format and dayfirst=True\n",
        "df['TS'] = pd.to_datetime(df['Timestamp'], format='%m/%d/%Y %H:%M', dayfirst=True)\n",
        "\n",
        "# Add 'day of week', 'hour', 'month', and 'year' columns\n",
        "df['Day of Week'] = df['TS'].dt.strftime('%a')\n",
        "df['Hour'] = df['TS'].dt.hour\n",
        "df['Month'] = df['TS'].dt.month\n",
        "df['Year'] = df['TS'].dt.year\n",
        "\n",
        "# Display the first few rows of the updated DataFrame\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "HCapSvJwNDtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data consistency check"
      ],
      "metadata": {
        "id": "s5ILjzhuOMhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "pnBPX5gccN0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum().plot.bar()"
      ],
      "metadata": {
        "id": "CytD0drVOJ_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a figure with two subplots\n",
        "fig, axes = plt.subplots(1,3 , figsize=(16, 6))\n",
        "\n",
        "# Trend of Appliances energy Kwh over Kitchen Temp °C\n",
        "sns.lineplot(ax=axes[0], x='Kitchen Temp °C', y='Appliances energy Kwh', data=df)\n",
        "axes[0].set_title('Appliances Energy Kwh vs. Kitchen Temp °C')\n",
        "\n",
        "# Trend of Lights energy Kwh over Kitchen Temp °C\n",
        "sns.lineplot(ax=axes[0], x='Kitchen Temp °C', y='Lights energy Kwh', data=df)\n",
        "axes[0].set_title('Lights Energy Kwh vs. Kitchen Temp °C')\n",
        "\n",
        "# Trend of Appliances energy Kwh over Hour\n",
        "sns.lineplot(ax=axes[1], x='Hour', y='Appliances energy Kwh', data=df)\n",
        "axes[1].set_title('Appliances Energy Kwh vs. Hour of the Day')\n",
        "\n",
        "# Trend of Lights energy Kwh over Hour\n",
        "sns.lineplot(ax=axes[1], x='Hour', y='Lights energy Kwh', data=df)\n",
        "axes[1].set_title('Lights Energy Kwh vs. Hour of the Day')\n",
        "\n",
        "\n",
        "# Trend of Appliances energy Kwh over Day of Week\n",
        "sns.lineplot(ax=axes[2], x='Day of Week', y='Appliances energy Kwh', data=df)\n",
        "axes[2].set_title('Appliances Energy Kwh vs. Day of week')\n",
        "\n",
        "# Trend of Lights energy Kwh over Day of Week\n",
        "sns.lineplot(ax=axes[2], x='Day of Week', y='Lights energy Kwh', data=df)\n",
        "axes[2].set_title('Lights Energy Kwh vs. Day of Week')\n",
        "\n",
        "\n",
        "# Show the plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DlJ_Zu_6OvYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the 'Timestamp' column as the index\n",
        "df.set_index('TS', inplace=True)\n",
        "\n",
        "# Select only numeric columns for the correlation matrix and pairplot\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(16, 12))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Plot the pairplot (scatter plots)\n",
        "sns.pairplot(df[numeric_cols], diag_kind='kde', plot_kws={'alpha':0.5, 's':30})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G6q4i0dQhl7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Convert 'TS' column to datetime\n",
        "# df['TS'] = pd.to_datetime(df['TS']) #This line causes error since TS is in index, not column\n",
        "\n",
        "# Reset the index to make 'TS' a column again\n",
        "df = df.reset_index()\n",
        "\n",
        "df['TS'] = pd.to_datetime(df['TS'])\n",
        "\n",
        "# Set the 'TS' column as the index\n",
        "df.set_index('TS', inplace=True)\n",
        "\n",
        "# Plot the trend chart\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Appliances energy Kwh'], label='Appliances energy Kwh')\n",
        "plt.plot(df['Lights energy Kwh'], label='Lights energy Kwh')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Energy Kwh')\n",
        "plt.title('Trend Chart of Appliances and Lights Energy Kwh vs TS')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "dLWV4j_Si3-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset for January\n",
        "january_df = df[df.index.month == 1]\n",
        "\n",
        "# Plot the trend chart for January\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(january_df['Appliances energy Kwh'], label='Appliances energy Kwh')\n",
        "plt.plot(january_df['Lights energy Kwh'], label='Lights energy Kwh')\n",
        "plt.plot(january_df['Outside Temperature  °C'], label='Outside Temperature', linestyle='--')\n",
        "plt.plot(january_df['Outside RH%'], label='Outside RH%', linestyle='--')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Trend Chart of Energy Usage and Outside Conditions vs TS (January)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K2gAHzGxjrJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter the dataset for June\n",
        "may_df = df[df.index.month == 5]\n",
        "\n",
        "# Plot the trend chart for May\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(may_df['Appliances energy Kwh'], label='Appliances energy Kwh')\n",
        "plt.plot(may_df['Lights energy Kwh'], label='Lights energy Kwh')\n",
        "plt.plot(may_df['Outside Temperature  °C'], label='Outside Temperature', linestyle='--')\n",
        "plt.plot(may_df['Outside RH%'], label='Outside RH%', linestyle='--')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Values')\n",
        "plt.title('Trend Chart of Energy Usage and Outside Conditions vs TS (May)')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tXJbXWpsjw7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Convert 'TS' column to datetime\n",
        "# df['TS'] = pd.to_datetime(df['TS']) #This line causes error since TS is in index, not column\n",
        "\n",
        "# Reset the index to make 'TS' a column again\n",
        "df = df.reset_index()\n",
        "\n",
        "# Now you can access 'TS' as a column\n",
        "df['TS'] = pd.to_datetime(df['TS'])\n",
        "\n",
        "# Extract the time part from 'TS' and create a new column 'Time'\n",
        "df['Time'] = df['TS'].dt.time\n",
        "\n",
        "# Display the dataframe with the new 'Time' column\n",
        "print(df.head())\n",
        "\n",
        "df.Time"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "RSse-bq5n352"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Length of dataframe\n",
        "len(df)"
      ],
      "metadata": {
        "id": "H4X0laH8oKpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Reset the index to make 'TS' a column again\n",
        "df = df.reset_index()\n",
        "\n",
        "# Now you can access 'TS' as a column\n",
        "df['TS'] = pd.to_datetime(df['TS'])\n",
        "\n",
        "# Set the 'TS' column as the index\n",
        "df.set_index('TS', inplace=True)\n",
        "\n",
        "# Select relevant columns for the model\n",
        "data = df[['Outside Temperature °C', 'Appliances energy Kwh']]\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_dataset(data, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        a = data[i:(i + time_step), 0]\n",
        "        X.append(a)\n",
        "        y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the dataset with a time step of 24 (assuming hourly data)\n",
        "time_step = 24\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "test_size = len(X) - train_size\n",
        "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
        "y_train, y_test = y[0:train_size], y[train_size:len(y)]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and actual values\n",
        "train_predict = scaler.inverse_transform(np.concatenate((np.zeros((train_predict.shape[0], 1)), train_predict), axis=1))[:, 1]\n",
        "test_predict = scaler.inverse_transform(np.concatenate((np.zeros((test_predict.shape[0], 1)), test_predict), axis=1))[:, 1]\n",
        "y_train = scaler.inverse_transform(np.concatenate((np.zeros((len(y_train), 1)), y_train.reshape(-1, 1)), axis=1))[:, 1]\n",
        "y_test = scaler.inverse_transform(np.concatenate((np.zeros((len(y_test), 1)), y_test.reshape(-1, 1)), axis=1))[:, 1]\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index[-len(y_test):], y_test, label='Actual Appliances energy Kwh')\n",
        "plt.plot(df.index[-len(y_test):], test_predict, label='Predicted Appliances energy Kwh')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Appliances energy Kwh')\n",
        "plt.title('Actual vs Predicted Appliances energy Kwh')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "G1x8aGdJpO-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Reset the index to make 'TS' a column again if it's currently the index\n",
        "if df.index.name == 'TS':\n",
        "    df = df.reset_index()\n",
        "\n",
        "# Ensure 'TS' is a datetime column\n",
        "df['TS'] = pd.to_datetime(df['TS'])\n",
        "\n",
        "# Set the 'TS' column as the index (if desired)\n",
        "df.set_index('TS', inplace=True)\n",
        "\n",
        "# Now you can access columns by name\n",
        "data = df[['Outside Temperature  °C', 'Appliances energy Kwh']]\n",
        "# Note: Corrected column name\n",
        "\n",
        "# ... rest of your code ...\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Prepare the data for LSTM\n",
        "def create_dataset(data, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        a = data[i:(i + time_step), 0]\n",
        "        X.append(a)\n",
        "        y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the dataset with a time step of 24 (assuming hourly data)\n",
        "time_step = 24\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "test_size = len(X) - train_size\n",
        "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
        "y_train, y_test = y[0:train_size], y[train_size:len(y)]\n",
        "\n",
        "# LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and actual values\n",
        "train_predict = scaler.inverse_transform(np.concatenate((np.zeros((train_predict.shape[0], 1)), train_predict), axis=1))[:, 1]\n",
        "test_predict = scaler.inverse_transform(np.concatenate((np.zeros((test_predict.shape[0], 1)), test_predict), axis=1))[:, 1]\n",
        "y_train = scaler.inverse_transform(np.concatenate((np.zeros((len(y_train), 1)), y_train.reshape(-1, 1)), axis=1))[:, 1]\n",
        "y_test = scaler.inverse_transform(np.concatenate((np.zeros((len(y_test), 1)), y_test.reshape(-1, 1)), axis=1))[:, 1]\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df.index[-len(y_test):], y_test, label='Actual Appliances energy Kwh')\n",
        "plt.plot(df.index[-len(y_test):], test_predict, label='Predicted Appliances energy Kwh')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Appliances energy Kwh')\n",
        "plt.title('Actual vs Predicted Appliances energy Kwh')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "oThufCfHqAK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# summarize history for Loss/MSE\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "# Assign the output of model.fit to history\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss/MSE')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"LSTM_loss1.png\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "QKfMIi3St26z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation**:\n",
        "\n",
        "Initially, both the training and testing losses are high, with the training loss starting above 16000 and the testing loss starting above 11000. As the number of epochs increases, both losses decrease rapidly at first and then level off. The training loss stabilizes around 11000, while the testing loss stabilizes around 8000.\n",
        "\n",
        "This graph indicates that the model's performance improves over time, as evidenced by the decreasing loss values. However, the training loss remains higher than the testing loss, which could suggest that the model is overfitting to the training data. The testing loss stabilizing at a lower value indicates that the model generalizes better to unseen data"
      ],
      "metadata": {
        "id": "mA9r6a8Sw77F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Prepare the data for LSTM with Conv1D\n",
        "def create_dataset(data, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        a = data[i:(i + time_step), 0]\n",
        "        X.append(a)\n",
        "        y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the dataset with a time step of 24 (assuming hourly data)\n",
        "time_step = 24\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "test_size = len(X) - train_size\n",
        "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
        "y_train, y_test = y[0:train_size], y[train_size:len(y)]\n",
        "\n",
        "# Build the LSTM model with Conv1D layer\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(time_step, 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and actual values\n",
        "train_predict = scaler.inverse_transform(np.concatenate((np.zeros((train_predict.shape[0], 1)), train_predict), axis=1))[:, 1]\n",
        "test_predict = scaler.inverse_transform(np.concatenate((np.zeros((test_predict.shape[0], 1)), test_predict), axis=1))[:, 1]\n",
        "y_train = scaler.inverse_transform(np.concatenate((np.zeros((len(y_train), 1)), y_train.reshape(-1, 1)), axis=1))[:, 1]\n",
        "y_test = scaler.inverse_transform(np.concatenate((np.zeros((len(y_test), 1)), y_test.reshape(-1, 1)), axis=1))[:, 1]\n",
        "\n",
        "# Plot the results for the months of June and July\n",
        "june_july_test_indices = df.index[(df.index.month == 6) | (df.index.month == 7)]\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(june_july_test_indices, y_test, label='Actual Appliances energy Kwh')\n",
        "plt.plot(june_july_test_indices, test_predict, label='Predicted Appliances energy Kwh')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Appliances energy Kwh')\n",
        "plt.title('Actual vs Predicted Appliances energy Kwh for June and July')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the plot\n",
        "fig_acc.savefig(\"LSTM_Conv1D_predictions_June_July.png\")\n"
      ],
      "metadata": {
        "id": "NtjIQR12x_v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'data' is the original dataframe with a datetime index and the actual values\n",
        "original_df = pd.DataFrame(data, columns=['Timestamp', 'Actual Appliances energy Kwh'])\n",
        "original_df.index = pd.to_datetime(original_df.index)\n",
        "\n",
        "# Creating dataframes for predictions\n",
        "train_predict_df = pd.DataFrame(train_predict, index=original_df.index[:len(train_predict)], columns=['Train Predicted Appliances energy Kwh'])\n",
        "test_predict_df = pd.DataFrame(test_predict, index=original_df.index[len(train_predict):(len(train_predict) + len(test_predict))], columns=['Test Predicted Appliances energy Kwh'])\n",
        "\n",
        "# Concatenate original and predicted dataframes\n",
        "merged_df = pd.concat([original_df, train_predict_df, test_predict_df], axis=1)\n",
        "\n",
        "# Export to Excel\n",
        "merged_df.to_excel(\"original_and_predicted_data.xlsx\", index=True)\n",
        "\n",
        "print(\"The dataset has been successfully exported to 'original_and_predicted_data.xlsx'.\")\n"
      ],
      "metadata": {
        "id": "t0PBeXcq1l5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file from above\n",
        "\n",
        "from google.colab import files\n",
        "files.download('original_and_predicted_data.xlsx')\n"
      ],
      "metadata": {
        "id": "WbHvb0gI10dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Plot the results for the months of June and July\n",
        "# Filter for June and July in the original DataFrame (before create_dataset)\n",
        "june_july_indices = df.index[(df.index.month == 6) | (df.index.month == 7)]\n",
        "\n",
        "# Get the corresponding indices in y_test\n",
        "june_july_test_indices_bool = np.isin(df.index[-len(y_test):], june_july_indices)\n",
        "june_july_test_indices = df.index[-len(y_test):][june_july_test_indices_bool]\n",
        "june_july_y_test = y_test[june_july_test_indices_bool]\n",
        "june_july_test_predict = test_predict[june_july_test_indices_bool]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Use the filtered data for plotting\n",
        "plt.plot(june_july_test_indices, june_july_y_test, label='Actual Appliances energy Kwh')\n",
        "plt.plot(june_july_test_indices, june_july_test_predict, label='Predicted Appliances energy Kwh')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Appliances energy Kwh')\n",
        "plt.title('Actual vs Predicted Appliances energy Kwh for June and July')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the plot\n",
        "fig_acc.savefig(\"LSTM_Conv1D_predictions_June_July.png\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "OJR3mZnczxHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic statistical analysis"
      ],
      "metadata": {
        "id": "qqk4Qy0yqVQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Prepare the data for LSTM with Conv1D\n",
        "def create_dataset(data, time_step=1):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_step - 1):\n",
        "        a = data[i:(i + time_step), 0]\n",
        "        X.append(a)\n",
        "        y.append(data[i + time_step, 1])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the dataset with a time step of 24 (assuming hourly data)\n",
        "time_step = 24\n",
        "X, y = create_dataset(scaled_data, time_step)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_size = int(len(X) * 0.8)\n",
        "test_size = len(X) - train_size\n",
        "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
        "y_train, y_test = y[0:train_size], y[train_size:len(y)]\n",
        "\n",
        "# Build the LSTM model with Conv1D layer\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(time_step, 1)))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(LSTM(50, return_sequences=True))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(1))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and actual values\n",
        "train_predict = scaler.inverse_transform(np.concatenate((np.zeros((train_predict.shape[0], 1)), train_predict), axis=1))[:, 1]\n",
        "test_predict = scaler.inverse_transform(np.concatenate((np.zeros((test_predict.shape[0], 1)), test_predict), axis=1))[:, 1]\n",
        "y_train = scaler.inverse_transform(np.concatenate((np.zeros((len(y_train), 1)), y_train.reshape(-1, 1)), axis=1))[:, 1]\n",
        "y_test = scaler.inverse_transform(np.concatenate((np.zeros((len(y_test), 1)), y_test.reshape(-1, 1)), axis=1))[:, 1]\n",
        "\n",
        "# Create a dataframe for the y_test to align with the original data indices\n",
        "y_test_df = pd.DataFrame(y_test, index=original_df.index[-len(y_test):], columns=['Actual Appliances energy Kwh'])\n",
        "\n",
        "# Create a dataframe for the test predictions to align with the original data indices\n",
        "test_predict_df = pd.DataFrame(test_predict, index=original_df.index[-len(test_predict):], columns=['Predicted Appliances energy Kwh'])\n",
        "\n",
        "# Filter the indices for June and July\n",
        "june_july_indices = y_test_df[(y_test_df.index.month == 6) | (y_test_df.index.month == 7)].index\n",
        "\n",
        "# Plot the results for the months of June and July\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(june_july_indices, y_test_df.loc[june_july_indices], label='Actual Appliances energy Kwh')\n",
        "plt.plot(june_july_indices, test_predict_df.loc[june_july_indices], label='Predicted Appliances energy Kwh')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Appliances energy Kwh')\n",
        "plt.title('Actual vs Predicted Appliances energy Kwh for June and July')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig(\"LSTM_Conv1D_predictions_June_July.png\")\n"
      ],
      "metadata": {
        "id": "2LI-EkTH0WIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Pearson correlation between 'Appliances energy Kwh' and 'Outside Temperature  °C'\n",
        "\n",
        "corr, p_value = pearsonr(df['Appliances energy Kwh'], df['Outside Temperature  °C'])\n",
        "print(f\"Pearson correlation: {corr}, P-value: {p_value}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "_urvi_gJ0cYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Pearson correlation of 0.099 suggests a very weak positive linear relationship between the two variables you're analyzing. This means that as one variable increases, the other variable tends to increase slightly as well, but the relationship is not strong or reliable.\n",
        "\n",
        "The P-value of 2.6247e-44 is extremely small, indicating that the observed correlation is highly statistically significant."
      ],
      "metadata": {
        "id": "siSZam-W1JBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "# Example: Linear regression of 'Appliances energy Kwh' on 'Outside Temperature °C'\n",
        "X = df['Outside Temperature  °C']\n",
        "y = df['Appliances energy Kwh']\n",
        "X = sm.add_constant(X)  # Adds a constant term to the predictor\n",
        "\n",
        "model = sm.OLS(y, X).fit()\n",
        "predictions = model.predict(X)\n",
        "\n",
        "# Print out the statistics\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "gRt9t8qrdsJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Pearson correlation between 'Appliances energy Kwh' and 'Outside RH%'\n",
        "\n",
        "corr, p_value = pearsonr(df['Appliances energy Kwh'], df['Outside RH%'])\n",
        "print(f\"Pearson correlation: {corr}, P-value: {p_value}\")"
      ],
      "metadata": {
        "id": "1IOOTZnO0nrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "weak negative linear relationship between the two variables you're analyzing. In practical terms, this means that as one variable increases, the other tends to slightly decrease, but the relationship is not strong or consistent.\n",
        "\n",
        "The P-value of 1.0775e-102 is extremely small, indicating that the observed correlation is highly statistically significant"
      ],
      "metadata": {
        "id": "W3k_lXpj1DmP"
      }
    }
  ]
}